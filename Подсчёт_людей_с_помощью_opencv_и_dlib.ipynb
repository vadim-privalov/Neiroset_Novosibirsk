{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Подсчёт людей с помощью opencv и dlib",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vadim-privalov/Neiroset_Novosibirsk/blob/main/%D0%9F%D0%BE%D0%B4%D1%81%D1%87%D1%91%D1%82_%D0%BB%D1%8E%D0%B4%D0%B5%D0%B9_%D1%81_%D0%BF%D0%BE%D0%BC%D0%BE%D1%89%D1%8C%D1%8E_opencv_%D0%B8_dlib.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_M2L9J2AX_dR"
      },
      "source": [
        "<h1 style=\"text-align: center;\"><b>Подсчёт людей с помощью opencv и dlib</b></h1>\n",
        "\n",
        "\n",
        "<h1 style=\"text-align: center;\"><b>Counting people with opencv and dlib</b></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFNyWRZJbmxu"
      },
      "source": [
        "В этом уроке мы будем считать сколько людей пересекли линию, сколько прошли вверх и вниз. Находить и выделять людей мы будем с помощью opencv используя сеть MobileNet SSD, а отслеживать их перемещение будем с помощью корреляционного трекера с библиотеки dlib. Не ждите очень хорошего качества выделения и отслеживания объектов. Сеть MobileNet SSD относительно простая и корреляционный трекер не всегда хорошо обрабатывает окклюзии.\n",
        "\n",
        "In this tutorial we will count how many people crossed the line, how many went up and down. We will find and select people with opencv using the MobileNet SSD network, and track their movement with the correlation tracker from the dlib library. Don't expect very good extraction and tracking quality. The MobileNet SSD network is relatively simple and the correlation tracker doesn't always handle occlusions well.\n",
        "\n",
        "\n",
        "Это более простой урок для демонстрации работы принципа, по которому происходит посчёт людей. В будущих уроках мы усовершенствуем эту схему и будем использовать более мощные и точные иструменты.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "This is a simpler lesson to demonstrate how the principle behind how people are counted works. In future lessons, we will improve this scheme and use more powerful and accurate tools.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwwAzvHic2yS"
      },
      "source": [
        "Скачиваем файлы, необходимые для урока, и устанавливаем корневую папку проекта.\n",
        "\n",
        "Download the files needed for the lesson, and install the root folder of the project.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-bkJ8KSDL2B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3145ebc0-398b-460a-e893-6de406e37388"
      },
      "source": [
        "!wget http://dataudt.ru/datasets/cv/Lesson_50.Counting_people_opencv_dlib.zip\n",
        "!unzip -qq Lesson_50.Counting_people_opencv_dlib.zip\n",
        "%cd /content/Lesson_50.Counting_people_opencv_dlib/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-20 09:53:00--  http://dataudt.ru/datasets/cv/Lesson_50.Counting_people_opencv_dlib.zip\n",
            "Resolving dataudt.ru (dataudt.ru)... 37.228.117.130\n",
            "Connecting to dataudt.ru (dataudt.ru)|37.228.117.130|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 26920482 (26M) [application/zip]\n",
            "Saving to: ‘Lesson_50.Counting_people_opencv_dlib.zip’\n",
            "\n",
            "Lesson_50.Counting_ 100%[===================>]  25.67M  12.8MB/s    in 2.0s    \n",
            "\n",
            "2022-02-20 09:53:03 (12.8 MB/s) - ‘Lesson_50.Counting_people_opencv_dlib.zip’ saved [26920482/26920482]\n",
            "\n",
            "/content/Lesson_50.Counting_people_opencv_dlib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MWIfsi9dB1Y"
      },
      "source": [
        "Импортируем необходимые модули.\n",
        "\n",
        "Import the necessary modules.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlyzB6CTD3rX"
      },
      "source": [
        "# дополнительные модули для учёта и отслеживания объектов\n",
        "# additional modules for object accounting and tracking\n",
        "from Objects.centroidtracker import CentroidTracker\n",
        "from Objects.trackableobject import TrackableObject\n",
        "# библиотека для корреляционного трекера\n",
        "# library for the correlation tracker\n",
        "import dlib\n",
        "# модуль для отображения картинок в colab\n",
        "# module for displaying pictures in colab\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import time"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_pvIdO556Mj"
      },
      "source": [
        "Укажем путь к модели, входным видео, и зададим параметры.\n",
        "\n",
        "Let's specify the path to the model, the input video, and set the parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKrkg_t7EyT1"
      },
      "source": [
        "args = {\n",
        "    \"prototxt\": \"Model/MobileNetSSD_deploy.prototxt\", # параметры модели\n",
        "                                                        # model parameters\n",
        "    \"model\": \"Model/MobileNetSSD_deploy.caffemodel\", # модель нейронной сети\n",
        "                                                    # neural network model\n",
        "    \"input\": \"Videos_in/example_02.mp4\", # путь входного видео\n",
        "                                        # input video path\n",
        "    \"output\": \"Videos_out/output_02.avi\", # путь для выходного видео\n",
        "                                        # way for the output video\n",
        "    \"confidence\": 0.4, # уровень достоверности обнаружения объектов\n",
        "                        # level of object detection reliability\n",
        "    \"skip_frames\": 20  # количество кадров, после которого происходит обнаружение\n",
        "                    # number of frames after which detection occurs\n",
        "}"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJICJFfl5NKF"
      },
      "source": [
        "Укажем координаты точек линии подсчёта. И зададим функцию для расчёта точек пересечения линии.\n",
        "\n",
        "Specify the coordinates of the counting line points. And set a function to calculate the intersection points of the line."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnsahBsq5IKW"
      },
      "source": [
        "line_x1, line_y1 = 45, 158\n",
        "line_x2, line_y2 = 550, 335\n",
        "\n",
        "def border_line(x):\n",
        "    return int((x-line_x1)/(line_x2-line_x1)*(line_y2-line_y1)+line_y1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ML6UDB4vFvJs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7603fda-7773-4579-eb59-52e79d806f7d"
      },
      "source": [
        "# инициализируем список меток классов, которые\n",
        "# MobileNet SSD была обучена обнаруживать\n",
        "# initialize the list of class labels MobileNet SSD was trained to\n",
        "# detect\n",
        "CLASSES = [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\",\n",
        "\t\"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\",\n",
        "\t\"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\",\n",
        "\t\"sofa\", \"train\", \"tvmonitor\"]\n",
        "\n",
        "# загружаем нашу сохранённую модель с диска\n",
        "# load our serialized model from disk\n",
        "print(\"[INFO] loading model...\")\n",
        "net = cv2.dnn.readNetFromCaffe(args[\"prototxt\"], args[\"model\"])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] loading model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VuLnIdAF6Gw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58eaf060-0318-461c-c684-7f13df069053"
      },
      "source": [
        "# используем ссылку на видео файл\n",
        "# и захватываем видео\n",
        "# grab a reference to the video file\n",
        "print(\"[INFO] opening video file...\")\n",
        "vs = cv2.VideoCapture(args[\"input\"])\n",
        "\n",
        "# инициализируем средство записи видео \n",
        "# (при необходимости мы создадим его экземпляр позже)\n",
        "# initialize the video writer (we'll instantiate later if need be)\n",
        "writer = None\n",
        "\n",
        "# инициализируем размеры кадра \n",
        "# (мы установим их, как только прочитаем первый кадр из видео)\n",
        "# initialize the frame dimensions (we'll set them as soon as we read\n",
        "# the first frame from the video)\n",
        "W = None\n",
        "H = None"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] opening video file...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fH5savMYGDw-"
      },
      "source": [
        "# создаем экземпляр нашего трекера центроидов, \n",
        "# затем инициализируем список для хранения каждого из наших \n",
        "# трекеров корреляции dlib, за которым следует словарь для \n",
        "# сопоставления каждого уникального идентификатора объекта\n",
        "# instantiate our centroid tracker, then initialize a list to store\n",
        "# each of our dlib correlation trackers, followed by a dictionary to\n",
        "# map each unique object ID to a TrackableObject\n",
        "ct = CentroidTracker(maxDisappeared=40, maxDistance=50)\n",
        "trackers = []\n",
        "trackableObjects = {}\n",
        "\n",
        "# инициализируем общее количество кадров, обработанных \n",
        "# на данный момент, вместе с общим количеством объектов,\n",
        "# которые переместились вверх или вниз\n",
        "# initialize the total number of frames processed thus far, along\n",
        "# with the total number of objects that have moved either up or down\n",
        "totalFrames = 0\n",
        "totalDown = 0\n",
        "totalUp = 0"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6msLhX-eGPUw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16d9c256-a1a9-41a4-94c6-8a199b059f5c"
      },
      "source": [
        "%%time\n",
        "# засечём время для оценки fps \n",
        "# time to estimate fps \n",
        "start_time = time.time()\n",
        "\n",
        "# перебираем кадры из видеопотока\n",
        "# loop over frames from the video stream\n",
        "while True:\n",
        "    # считываем кадр с видео\n",
        "    # grab the next frame \n",
        "    success, frame = vs.read()\n",
        "    \n",
        "    if success:\n",
        "        # конвертируем кадр из BGR в RGB для dlib\n",
        "        # convert the frame from BGR to RGB for dlib\n",
        "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # если размеры фрейма пустые, установим их\n",
        "        # if the frame dimensions are empty, set them\n",
        "        if W is None or H is None:\n",
        "            (H, W) = frame.shape[:2]\n",
        "\n",
        "        # инициализируем запись\n",
        "        # initialize the writer\n",
        "        if args[\"output\"] is not None and writer is None:\n",
        "            fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
        "            writer = cv2.VideoWriter(args[\"output\"], fourcc, 30,\n",
        "                (W, H), True)\n",
        "\n",
        "        # инициализируем текущий статус вместе с нашим списком \n",
        "        # ограничивающих рамок, возвращаемыми либо (1) детектором\n",
        "        # объектов, либо (2) трекерами корреляции\n",
        "        # initialize the current status along with our list of bounding\n",
        "        # box rectangles returned by either (1) our object detector or\n",
        "        # (2) the correlation trackers\n",
        "        status = \"Waiting\"\n",
        "        rects = []\n",
        "\n",
        "        # проверяем, следует ли запускать более дорогостоящий в \n",
        "        # вычислительном отношении метод обнаружения объектов,\n",
        "        # чтобы помочь нашему трекеру\n",
        "        # check to see if we should run a more computationally expensive\n",
        "        # object detection method to aid our tracker\n",
        "        if totalFrames % args[\"skip_frames\"] == 0:\n",
        "            # устанавливаем статус и инициализируем \n",
        "            # наш новый набор трекеров объектов\n",
        "            # set the status and initialize our new set of object trackers\n",
        "            status = \"Detecting\"\n",
        "            trackers = []\n",
        "\n",
        "            # преобразовываем кадр, пропускаем его через сеть и получаем обнаружение\n",
        "            # convert the frame to a blob and pass the blob through the\n",
        "            # network and obtain the detections\n",
        "            blob = cv2.dnn.blobFromImage(frame, 0.007843, (W, H), 127.5)\n",
        "            net.setInput(blob)\n",
        "            detections = net.forward()\n",
        "\n",
        "            # перебираем что обнаружили\n",
        "            # loop over the detections\n",
        "            for i in np.arange(0, detections.shape[2]):\n",
        "                # извлекаем достоверность (т.е. вероятность), связанную с прогнозом\n",
        "                # extract the confidence (i.e., probability) associated\n",
        "                # with the prediction\n",
        "                confidence = detections[0, 0, i, 2]\n",
        "\n",
        "                # отфильтровываем обнаружения меньше\n",
        "                # минимальной достоверности\n",
        "                # filter out weak detections by requiring a minimum\n",
        "                # confidence\n",
        "                if confidence > args[\"confidence\"]:\n",
        "                    # извлекаем индекс метки класса из списка обнаружений\n",
        "                    # extract the index of the class label from the\n",
        "                    # detections list\n",
        "                    idx = int(detections[0, 0, i, 1])\n",
        "\n",
        "                    # если метка класса не человек, игнорируем её\n",
        "                    # if the class label is not a person, ignore it\n",
        "                    if CLASSES[idx] != \"person\":\n",
        "                        continue\n",
        "\n",
        "                    # вычисляем x, y координаты ограничивающего \n",
        "                    # прямоугольника объекта\n",
        "                    # compute the (x, y)-coordinates of the bounding box\n",
        "                    # for the object\n",
        "                    box = detections[0, 0, i, 3:7] * np.array([W, H, W, H])\n",
        "                    (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "\n",
        "                    # построим dlib прямоугольный объект из координат \n",
        "                    # ограничивающего прямоугольника, а затем \n",
        "                    # запустим трекер корреляции dlib\n",
        "                    # construct a dlib rectangle object from the bounding\n",
        "                    # box coordinates and then start the dlib correlation\n",
        "                    # tracker\n",
        "                    tracker = dlib.correlation_tracker()\n",
        "                    rect = dlib.rectangle(startX, startY, endX, endY)\n",
        "                    tracker.start_track(rgb, rect)\n",
        "\n",
        "                    # добавляем трекер в наш список трекеров, чтобы мы могли \n",
        "                    # использовать его во время пропуска кадров\n",
        "                    # add the tracker to our list of trackers so we can\n",
        "                    # utilize it during skip frames\n",
        "                    trackers.append(tracker)\n",
        "\n",
        "        # в противном случае мы должны использовать наши объектные трекеры, \n",
        "        # а не объектные детекторы, чтобы получить более высокую \n",
        "        # пропускную способность обработки кадров\n",
        "        # otherwise, we should utilize our object *trackers* rather than\n",
        "        # object *detectors* to obtain a higher frame processing throughput\n",
        "        else:\n",
        "            # перебираем трекеры\n",
        "            # loop over the trackers\n",
        "            for tracker in trackers:\n",
        "                # установим статус нашей системы как «отслеживание», \n",
        "                # а не «ожидание» или «обнаружение»\n",
        "                # set the status of our system to be 'tracking' rather\n",
        "                # than 'waiting' or 'detecting'\n",
        "                status = \"Tracking\"\n",
        "\n",
        "                # обновим трекер и получим обновленную позицию\n",
        "                # update the tracker and grab the updated position\n",
        "                tracker.update(rgb)\n",
        "                pos = tracker.get_position()\n",
        "\n",
        "                # распаковываем координаты позиции объекта \n",
        "                # unpack the position object\n",
        "                startX = int(pos.left())\n",
        "                startY = int(pos.top())\n",
        "                endX = int(pos.right())\n",
        "                endY = int(pos.bottom())\n",
        "\n",
        "                # добавляем координаты ограничивающего прямоугольника \n",
        "                # в список прямоугольников\n",
        "                # add the bounding box coordinates to the rectangles list\n",
        "                rects.append((startX, startY, endX, endY))\n",
        "\n",
        "        # рисуем горизонтальную линию в центре кадра \n",
        "        # как только объект пересекает эту линию, мы определяем, \n",
        "        # двигался ли он вверх или вниз\n",
        "        # draw a horizontal line in the center of the frame -- once an\n",
        "        # object crosses this line we will determine whether they were\n",
        "        # moving 'up' or 'down'\n",
        "        cv2.line(frame, (line_x1, line_y1), (line_x2, line_y2), (0, 255, 255), 2)\n",
        "\n",
        "        # используем трекер центроидов, чтобы связать (1) старые центроиды\n",
        "        # объектов с (2) вновь вычисленными центроидами объектов\n",
        "        # use the centroid tracker to associate the (1) old object\n",
        "        # centroids with (2) the newly computed object centroids\n",
        "        objects = ct.update(rects)\n",
        "\n",
        "        # перебираем отслеживаемые объекты\n",
        "        # loop over the tracked objects\n",
        "        for (objectID, centroid) in objects.items():\n",
        "            # проверим, существует ли отслеживаемый объект \n",
        "            # для текущего идентификатора объекта\n",
        "            # check to see if a trackable object exists for the current\n",
        "            # object ID\n",
        "            to = trackableObjects.get(objectID, None)\n",
        "\n",
        "            # если нет существующего отслеживаемого объекта, создим его\n",
        "            # if there is no existing trackable object, create one\n",
        "            if to is None:\n",
        "                to = TrackableObject(objectID, centroid)\n",
        "\n",
        "            # в противном случае существует отслеживаемый объект, поэтому \n",
        "            # мы можем использовать его для определения направления\n",
        "            # otherwise, there is a trackable object so we can utilize it\n",
        "            # to determine direction\n",
        "            else:\n",
        "                # разница между координатой y текущего центроида и средним \n",
        "                # значением предыдущих центроидов укажет нам, в каком \n",
        "                # направлении движется объект \n",
        "                # (отрицательная для 'вверх' и положительная для 'вниз')\n",
        "                # the difference between the y-coordinate of the *current*\n",
        "                # centroid and the mean of *previous* centroids will tell\n",
        "                # us in which direction the object is moving (negative for\n",
        "                # 'up' and positive for 'down')\n",
        "                y = [c[1] for c in to.centroids]\n",
        "                direction = centroid[1] - np.mean(y)\n",
        "                to.centroids.append(centroid)\n",
        "\n",
        "                # проверяем, посчитан ли объект\n",
        "                # check to see if the object has been counted or not\n",
        "                if not to.counted:\n",
        "                    # если направление отрицательное (указывает на то, \n",
        "                    # что объект движется вверх) и центроид\n",
        "                    #  выше центральной линии, то считаем объект\n",
        "                    # if the direction is negative (indicating the object\n",
        "                    # is moving up) AND the centroid is above the center\n",
        "                    # line, count the object\n",
        "                    border_y = border_line(centroid[0])\n",
        "                    if direction < 0 and centroid[1] < border_y:\n",
        "                        totalUp += 1\n",
        "                        to.counted = True\n",
        "\n",
        "                    # если направление положительное (указывает на то, \n",
        "                    # что объект движется вниз) и центроид \n",
        "                    # ниже центральной линии, то считаем объект\n",
        "                    # if the direction is positive (indicating the object\n",
        "                    # is moving down) AND the centroid is below the\n",
        "                    # center line, count the object\n",
        "                    elif direction > 0 and centroid[1] > border_y:\n",
        "                        totalDown += 1\n",
        "                        to.counted = True\n",
        "\n",
        "            # сохраняем отслеживаемый объект в нашем словаре\n",
        "            # store the trackable object in our dictionary\n",
        "            trackableObjects[objectID] = to\n",
        "\n",
        "            # рисуем как ID объекта, так и \n",
        "            # центроид объекта на выходном кадре\n",
        "            # draw both the ID of the object and the centroid of the\n",
        "            # object on the output frame\n",
        "            text = \"ID {}\".format(objectID)\n",
        "            cv2.putText(frame, text, (centroid[0] - 10, centroid[1] - 10),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "            cv2.circle(frame, (centroid[0], centroid[1]), 4, (0, 255, 0), -1)\n",
        "\n",
        "        # создаем кортеж информации, который мы будем отображать в кадре\n",
        "        # construct a tuple of information we will be displaying on the\n",
        "        # frame\n",
        "        info = [\n",
        "            (\"Up\", totalUp),\n",
        "            (\"Down\", totalDown),\n",
        "            (\"Status\", status),\n",
        "        ]\n",
        "\n",
        "        # перебираем информационные кортежи и рисуем их на нашем фрейме\n",
        "        # loop over the info tuples and draw them on our frame\n",
        "        for (i, (k, v)) in enumerate(info):\n",
        "            text = \"{}: {}\".format(k, v)\n",
        "            cv2.putText(frame, text, (10, H - ((i * 20) + 20)),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)\n",
        "\n",
        "        # записываем выходной кадр в видео\n",
        "        # record the output frame in the video\n",
        "        writer.write(frame)\n",
        "\n",
        "        # Не будем отображать выходное изображение\n",
        "        # так как colab не выводит поток как видео,\n",
        "        # а отображает ленту картинок, что очень\n",
        "        # снижает производительность.\n",
        "        # Чтобы посмотреть как отработала наша \n",
        "        # программа воспользуемся кодом ниже\n",
        "        # cv2_imshow(frame)\n",
        "        # Will not display the output image\n",
        "        # since colab does not output the stream as video,\n",
        "        # but displays a feed of images, which is very\n",
        "        # decreases performance.\n",
        "        # To see how our\n",
        "        # program we will use the code below\n",
        "        # cv2_imshow (frame)\n",
        "\n",
        "        # increment the total number of frames processed thus far \n",
        "        # увеличиваем общее количество кадров, \n",
        "        # обработанных на данный момент\n",
        "        # increment the total number of frames processed thus far\n",
        "        # increase the total number of frames,\n",
        "        # currently processed\n",
        "        totalFrames += 1\n",
        "    else:\n",
        "        break\n",
        "\n",
        "# оценим производительность обработки в кадрах в секунду\n",
        "# let's estimate the processing performance in frames per second\n",
        "print('FPS: {:.2f}'.format(totalFrames/(time.time()-start_time)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FPS: 21.00\n",
            "CPU times: user 41 s, sys: 270 ms, total: 41.2 s\n",
            "Wall time: 29.3 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7wkrBSwnmnN"
      },
      "source": [
        "Чтобы посмотреть выходное видео непосредственно в Colab, установим модуль kora.\n",
        "\n",
        "To view the output video directly in Colab, install the kora module.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uJUP_lzs2Gc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bc4c91f-34c2-445a-b595-8b52126e790d"
      },
      "source": [
        "!pip install -U kora"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kora\n",
            "  Downloading kora-0.9.19-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 2.9 MB/s \n",
            "\u001b[?25hCollecting fastcore\n",
            "  Downloading fastcore-1.3.27-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from kora) (5.5.0)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (from fastcore->kora) (21.1.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from fastcore->kora) (21.3)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->kora) (2.6.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->kora) (5.1.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->kora) (1.0.18)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->kora) (57.4.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->kora) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->kora) (0.8.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->kora) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->kora) (4.4.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->kora) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->kora) (1.15.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->fastcore->kora) (3.0.7)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->kora) (0.7.0)\n",
            "Installing collected packages: fastcore, kora\n",
            "Successfully installed fastcore-1.3.27 kora-0.9.19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_SNEMFEoLue"
      },
      "source": [
        "Kora может восипроизводить видео только в формате .mp4, а выходные видео нашей программы в формате .avi. Конвертируем формат видео. Конвертация может занять какое-то время.\n",
        "\n",
        "\n",
        "Kora can only produce videos in .mp4 format, and our program's output videos are in .avi format. Convert the video format. Conversion may take some time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-XyIqdOouMQ"
      },
      "source": [
        "Для этого воспользуемся командой ffmpeg, укажем путь к выходному видео в .avi формате, и укажем имя, и путь для сконвертированного видео. (Если сконвертированное видео уже существует с таким именем, то нужно будет подтвердить замену указав \"y\" yes в поле.)\n",
        "\n",
        "\n",
        "To do this, use the command ffmpeg, specify the path to the output video in .avi format, and specify the name and path for the converted video. (If the converted video already exists with that name, you will need to confirm the replacement with \"y\" yes in the field.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhYbtMwnHTp-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adfb584a-848f-4dad-e880-4e7b4a47b73b"
      },
      "source": [
        "!ffmpeg -i Videos_out/output_02.avi Videos_out/output_02.mp4"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers\n",
            "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
            "  libavutil      55. 78.100 / 55. 78.100\n",
            "  libavcodec     57.107.100 / 57.107.100\n",
            "  libavformat    57. 83.100 / 57. 83.100\n",
            "  libavdevice    57. 10.100 / 57. 10.100\n",
            "  libavfilter     6.107.100 /  6.107.100\n",
            "  libavresample   3.  7.  0 /  3.  7.  0\n",
            "  libswscale      4.  8.100 /  4.  8.100\n",
            "  libswresample   2.  9.100 /  2.  9.100\n",
            "  libpostproc    54.  7.100 / 54.  7.100\n",
            "Input #0, avi, from 'Videos_out/output_02.avi':\n",
            "  Metadata:\n",
            "    encoder         : Lavf58.35.100\n",
            "  Duration: N/A, start: 0.000000, bitrate: N/A\n",
            "    Stream #0:0: Video: mjpeg (MJPG / 0x47504A4D), yuvj420p(pc, bt470bg/unknown/unknown), 640x360, 30 fps, 30 tbr, 30 tbn, 30 tbc\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (mjpeg (native) -> h264 (libx264))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;36m[libx264 @ 0x563cd5789e00] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
            "\u001b[1;36m[libx264 @ 0x563cd5789e00] \u001b[0mprofile High, level 3.0\n",
            "\u001b[1;36m[libx264 @ 0x563cd5789e00] \u001b[0m264 - core 152 r2854 e9a5903 - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=3 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
            "Output #0, mp4, to 'Videos_out/output_02.mp4':\n",
            "  Metadata:\n",
            "    encoder         : Lavf57.83.100\n",
            "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuvj420p(pc), 640x360, q=-1--1, 30 fps, 15360 tbn, 30 tbc\n",
            "    Metadata:\n",
            "      encoder         : Lavc57.107.100 libx264\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
            "\u001b[0;36m[mjpeg @ 0x563cd5788f00] \u001b[0m\u001b[1;31moverread 8\n",
            "\u001b[0m\u001b[0;36m[mjpeg @ 0x563cd5788f00] \u001b[0m\u001b[0;33mEOI missing, emulating\n",
            "\u001b[0mframe=  612 fps= 80 q=-1.0 Lsize=    2130kB time=00:00:20.30 bitrate= 859.5kbits/s speed=2.65x    \n",
            "video:2122kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.372808%\n",
            "\u001b[1;36m[libx264 @ 0x563cd5789e00] \u001b[0mframe I:3     Avg QP:19.64  size: 41424\n",
            "\u001b[1;36m[libx264 @ 0x563cd5789e00] \u001b[0mframe P:179   Avg QP:22.26  size:  9012\n",
            "\u001b[1;36m[libx264 @ 0x563cd5789e00] \u001b[0mframe B:430   Avg QP:29.76  size:  1011\n",
            "\u001b[1;36m[libx264 @ 0x563cd5789e00] \u001b[0mconsecutive B-frames:  2.5%  9.5%  6.4% 81.7%\n",
            "\u001b[1;36m[libx264 @ 0x563cd5789e00] \u001b[0mmb I  I16..4:  3.6% 89.2%  7.2%\n",
            "\u001b[1;36m[libx264 @ 0x563cd5789e00] \u001b[0mmb P  I16..4:  0.5%  5.5%  1.1%  P16..4: 27.9% 12.6% 14.9%  0.0%  0.0%    skip:37.6%\n",
            "\u001b[1;36m[libx264 @ 0x563cd5789e00] \u001b[0mmb B  I16..4:  0.1%  0.2%  0.1%  B16..8: 18.4%  2.9%  1.4%  direct: 0.7%  skip:76.1%  L0:51.5% L1:41.2% BI: 7.3%\n",
            "\u001b[1;36m[libx264 @ 0x563cd5789e00] \u001b[0m8x8 transform intra:76.9% inter:73.4%\n",
            "\u001b[1;36m[libx264 @ 0x563cd5789e00] \u001b[0mcoded y,uvDC,uvAC intra: 78.2% 68.6% 20.3% inter: 12.2% 9.4% 2.0%\n",
            "\u001b[1;36m[libx264 @ 0x563cd5789e00] \u001b[0mi16 v,h,dc,p: 26% 21% 45%  8%\n",
            "\u001b[1;36m[libx264 @ 0x563cd5789e00] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 16% 13% 51%  4%  2%  1%  2%  5%  5%\n",
            "\u001b[1;36m[libx264 @ 0x563cd5789e00] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 23% 14% 20%  6%  7%  8%  9%  8%  5%\n",
            "\u001b[1;36m[libx264 @ 0x563cd5789e00] \u001b[0mi8c dc,h,v,p: 51% 23% 23%  3%\n",
            "\u001b[1;36m[libx264 @ 0x563cd5789e00] \u001b[0mWeighted P-Frames: Y:0.0% UV:0.0%\n",
            "\u001b[1;36m[libx264 @ 0x563cd5789e00] \u001b[0mref P L0: 70.7% 11.9% 12.7%  4.8%\n",
            "\u001b[1;36m[libx264 @ 0x563cd5789e00] \u001b[0mref B L0: 84.2% 12.8%  3.0%\n",
            "\u001b[1;36m[libx264 @ 0x563cd5789e00] \u001b[0mref B L1: 94.1%  5.9%\n",
            "\u001b[1;36m[libx264 @ 0x563cd5789e00] \u001b[0mkb/s:851.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WqpwEhaorcK"
      },
      "source": [
        "Воспроизведём сконвертированное видео в Colab. Для этого дадим разрешение Google Cloud SDK на доступ к файлам.\n",
        "\n",
        "Play the converted video in Colab. To do this, give Google Cloud SDK permission to access the files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLuzyRDOs6-j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "58df00b6-b76f-49ed-f296-2d543cf5f714"
      },
      "source": [
        "from kora.drive import upload_public\n",
        "url = upload_public('Videos_out/output_02.mp4')\n",
        "\n",
        "from IPython.display import HTML\n",
        "HTML(f\"\"\"<video src={url} width=640 controls/>\"\"\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<video src=https://drive.google.com/uc?id=1cHSJOaGwHSv_Pk-95kT9bHfZx_kzKqRM width=640 controls/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}