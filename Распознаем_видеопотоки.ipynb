{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Распознаем видеопотоки",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vadim-privalov/Neiroset_Novosibirsk/blob/main/%D0%A0%D0%B0%D1%81%D0%BF%D0%BE%D0%B7%D0%BD%D0%B0%D0%B5%D0%BC_%D0%B2%D0%B8%D0%B4%D0%B5%D0%BE%D0%BF%D0%BE%D1%82%D0%BE%D0%BA%D0%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ej284ufNkkee"
      },
      "source": [
        "# Распознаем видеопотоки\n",
        "\n",
        "# Recognizing video streams\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjGwd3FQknQY"
      },
      "source": [
        "В этом уроке мы научимся распознавать текст с визитной карточки на видео. Для этого мы воспользуемся уже полученными знаниями распознавания видеопотоков, применим FFT для для обнаружения и удаления размытых и некачественных кадров и выведем все.\n",
        "\n",
        "In this lesson we will learn how to recognize text from a business card on a video. To do this we will use the knowledge we already have of video stream recognition, apply FFT to detect and remove blurry and low-quality frames and output everything."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQmq03XHknQa"
      },
      "source": [
        "Нам необходимо будет реализовать базовую вспомогательную функцию, которая позволит нам записать выходные данные нашего сценария распознавания видео на диск в виде отдельного выходного видео.\n",
        "\n",
        "We will need to implement a basic auxiliary function that will allow us to write the output of our video recognition scenario to disk as a separate output video."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5tTywfsODAf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a431e934-0f0a-4dab-9ef6-4f946fecd111"
      },
      "source": [
        "!wget http://dataudt.ru/datasets/cv/Lesson_17.OCR_video.zip\n",
        "!unzip Lesson_17.OCR_video.zip\n",
        "%cd /content/OCR_video\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-10 11:51:13--  http://dataudt.ru/datasets/cv/Lesson_17.OCR_video.zip\n",
            "Resolving dataudt.ru (dataudt.ru)... 37.228.117.130\n",
            "Connecting to dataudt.ru (dataudt.ru)|37.228.117.130|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 24947046 (24M) [application/zip]\n",
            "Saving to: ‘Lesson_17.OCR_video.zip’\n",
            "\n",
            "Lesson_17.OCR_video 100%[===================>]  23.79M  38.3MB/s    in 0.6s    \n",
            "\n",
            "2022-02-10 11:51:14 (38.3 MB/s) - ‘Lesson_17.OCR_video.zip’ saved [24947046/24947046]\n",
            "\n",
            "Archive:  Lesson_17.OCR_video.zip\n",
            "   creating: OCR_video/blur_detection/\n",
            "   creating: OCR_video/blur_detection/__pycache__/\n",
            "  inflating: OCR_video/blur_detection/__pycache__/blur_detector.cpython-36.pyc  \n",
            "  inflating: OCR_video/blur_detection/blur_detector.py  \n",
            "  inflating: OCR_video/helpers.py    \n",
            "  inflating: OCR_video/ocr_video.py  \n",
            "   creating: OCR_video/output/\n",
            "  inflating: OCR_video/output/ocr_video_output.avi  \n",
            "  inflating: OCR_video/rus.traineddata  \n",
            "   creating: OCR_video/video/\n",
            "  inflating: OCR_video/video/ex1.MOV  \n",
            "   creating: OCR_video/video_ocr/\n",
            "   creating: OCR_video/video_ocr/__pycache__/\n",
            "  inflating: OCR_video/video_ocr/__pycache__/visualization.cpython-36.pyc  \n",
            "  inflating: OCR_video/video_ocr/visualization.py  \n",
            "/content/OCR_video\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uSbSQ51knQc"
      },
      "source": [
        "но сперва настроим работу tesseract \n",
        "\n",
        "But first let's set up the tesseract \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmJVaRYq0s6Z",
        "outputId": "a41ae2b2-fce2-450a-ce4e-a5b56ad63ce0"
      },
      "source": [
        "!sudo apt install tesseract-ocr\n",
        "!pip install pytesseract"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following packages were automatically installed and are no longer required:\n",
            "  cuda-command-line-tools-10-0 cuda-command-line-tools-10-1\n",
            "  cuda-command-line-tools-11-0 cuda-compiler-10-0 cuda-compiler-10-1\n",
            "  cuda-compiler-11-0 cuda-cuobjdump-10-0 cuda-cuobjdump-10-1\n",
            "  cuda-cuobjdump-11-0 cuda-cupti-10-0 cuda-cupti-10-1 cuda-cupti-11-0\n",
            "  cuda-cupti-dev-11-0 cuda-documentation-10-0 cuda-documentation-10-1\n",
            "  cuda-documentation-11-0 cuda-documentation-11-1 cuda-gdb-10-0 cuda-gdb-10-1\n",
            "  cuda-gdb-11-0 cuda-gpu-library-advisor-10-0 cuda-gpu-library-advisor-10-1\n",
            "  cuda-libraries-10-0 cuda-libraries-10-1 cuda-libraries-11-0\n",
            "  cuda-memcheck-10-0 cuda-memcheck-10-1 cuda-memcheck-11-0 cuda-nsight-10-0\n",
            "  cuda-nsight-10-1 cuda-nsight-11-0 cuda-nsight-11-1 cuda-nsight-compute-10-0\n",
            "  cuda-nsight-compute-10-1 cuda-nsight-compute-11-0 cuda-nsight-compute-11-1\n",
            "  cuda-nsight-systems-10-1 cuda-nsight-systems-11-0 cuda-nsight-systems-11-1\n",
            "  cuda-nvcc-10-0 cuda-nvcc-10-1 cuda-nvcc-11-0 cuda-nvdisasm-10-0\n",
            "  cuda-nvdisasm-10-1 cuda-nvdisasm-11-0 cuda-nvml-dev-10-0 cuda-nvml-dev-10-1\n",
            "  cuda-nvml-dev-11-0 cuda-nvprof-10-0 cuda-nvprof-10-1 cuda-nvprof-11-0\n",
            "  cuda-nvprune-10-0 cuda-nvprune-10-1 cuda-nvprune-11-0 cuda-nvtx-10-0\n",
            "  cuda-nvtx-10-1 cuda-nvtx-11-0 cuda-nvvp-10-0 cuda-nvvp-10-1 cuda-nvvp-11-0\n",
            "  cuda-nvvp-11-1 cuda-samples-10-0 cuda-samples-10-1 cuda-samples-11-0\n",
            "  cuda-samples-11-1 cuda-sanitizer-11-0 cuda-sanitizer-api-10-1\n",
            "  cuda-toolkit-10-0 cuda-toolkit-10-1 cuda-toolkit-11-0 cuda-toolkit-11-1\n",
            "  cuda-tools-10-0 cuda-tools-10-1 cuda-tools-11-0 cuda-tools-11-1\n",
            "  cuda-visual-tools-10-0 cuda-visual-tools-10-1 cuda-visual-tools-11-0\n",
            "  cuda-visual-tools-11-1 default-jre dkms freeglut3 freeglut3-dev\n",
            "  keyboard-configuration libargon2-0 libcap2 libcryptsetup12\n",
            "  libdevmapper1.02.1 libfontenc1 libidn11 libip4tc0 libjansson4\n",
            "  libnvidia-cfg1-510 libnvidia-common-460 libnvidia-common-510\n",
            "  libnvidia-extra-510 libnvidia-fbc1-510 libnvidia-gl-510 libpam-systemd\n",
            "  libpolkit-agent-1-0 libpolkit-backend-1-0 libpolkit-gobject-1-0 libxfont2\n",
            "  libxi-dev libxkbfile1 libxmu-dev libxmu-headers libxnvctrl0 libxtst6\n",
            "  nsight-compute-2020.2.1 nsight-compute-2022.1.0 nsight-systems-2020.3.2\n",
            "  nsight-systems-2020.3.4 nsight-systems-2021.5.2 nvidia-dkms-510\n",
            "  nvidia-kernel-common-510 nvidia-kernel-source-510 nvidia-modprobe\n",
            "  nvidia-settings openjdk-11-jre policykit-1 policykit-1-gnome python3-xkit\n",
            "  screen-resolution-extra systemd systemd-sysv udev x11-xkb-utils\n",
            "  xserver-common xserver-xorg-core-hwe-18.04 xserver-xorg-video-nvidia-510\n",
            "Use 'sudo apt autoremove' to remove them.\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 4,795 kB of archives.\n",
            "After this operation, 15.8 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr-eng all 4.00~git24-0e00fe6-1.2 [1,588 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr-osd all 4.00~git24-0e00fe6-1.2 [2,989 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr amd64 4.00~git2288-10f4998a-2 [218 kB]\n",
            "Fetched 4,795 kB in 0s (20.4 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 155113 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_4.00~git24-0e00fe6-1.2_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (4.00~git24-0e00fe6-1.2) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_4.00~git24-0e00fe6-1.2_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (4.00~git24-0e00fe6-1.2) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.00~git2288-10f4998a-2_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.00~git2288-10f4998a-2) ...\n",
            "Setting up tesseract-ocr-osd (4.00~git24-0e00fe6-1.2) ...\n",
            "Setting up tesseract-ocr-eng (4.00~git24-0e00fe6-1.2) ...\n",
            "Setting up tesseract-ocr (4.00~git2288-10f4998a-2) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.8.tar.gz (14 kB)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from pytesseract) (7.1.2)\n",
            "Building wheels for collected packages: pytesseract\n",
            "  Building wheel for pytesseract (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytesseract: filename=pytesseract-0.3.8-py2.py3-none-any.whl size=14070 sha256=a76b11db6d9b4f2507bd7ccd4a6ff2e72ef822aed6b7bb876e6f8150cf37148d\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/89/b9/3f11250225d0f90e5454fcc30fd1b7208db226850715aa9ace\n",
            "Successfully built pytesseract\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Q4y1SOW029t"
      },
      "source": [
        "import pytesseract\n",
        "\n",
        "pytesseract.pytesseract.tesseract_cmd = r'/usr/bin/tesseract'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzx-_R2gk5ot"
      },
      "source": [
        "Если будем использовать русский язык, то переместим и этот файл \n",
        "\n",
        "If we use Russian, we will move this file as well \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ey2yrkEk06kR"
      },
      "source": [
        "!mv \"/content/OCR_video/rus.traineddata\" \"/usr/share/tesseract-ocr/4.00/tessdata/\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vr7APcj138T"
      },
      "source": [
        "# импортируем необходимые пакеты\n",
        "# import the necessary packages\n",
        "import numpy as np\n",
        "from imutils.video import VideoStream\n",
        "# для  перспективного преобразования (вид сверху) \n",
        "# for perspective transformation (top view)\n",
        "from imutils.perspective import four_point_transform \n",
        "from pytesseract import Output\n",
        "import pytesseract\n",
        "import imutils\n",
        "import time\n",
        "import cv2"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i8ZWV9Qk3pF"
      },
      "source": [
        "А теперь перейдем к самой функции:\n",
        "\n",
        "Now let's move on to the function itself:\n",
        "\n",
        "Функция `build` внутри класса `VideoOCROutputBuilder` принимает следующие параметры:\n",
        "\n",
        "The `build` function inside the `VideoOCROutputBuilder` class takes the following parameters:\n",
        "\n",
        "- `frame` - входной кадр из видео. the input frame from the video.\n",
        "- `card` - Визитная карточка после применения преобразования перспективы сверху вниз и обнаружения текста на карте. The calling card after applying a top-down perspective transformation and detecting text on the card.\n",
        "- `ocr` -  распознанный текст OCR. OCR recognized text .\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDOdVKWN0jSa"
      },
      "source": [
        "class VideoOCROutputBuilder:\n",
        "\tdef __init__(self, frame):\n",
        "        # храним размеры входного кадра\n",
        "        # store the input frame dimensions\n",
        "\t\tself.maxW = frame.shape[1]\n",
        "\t\tself.maxH = frame.shape[0]\n",
        "\n",
        "\tdef build(self, frame, card=None, ocr=None):\n",
        "        # получаем размеры входного кадра и инициализируем \n",
        "        # размеры изображения визитной карточки вместе\n",
        "        #  с размерами изображения OCR\n",
        "        # grab the input frame dimensions and  initialize the card\n",
        "\t\t# image dimensions along with the OCR image dimensions\n",
        "\t\t(frameH, frameW) = frame.shape[:2]\n",
        "\t\t(cardW, cardH) = (0, 0)\n",
        "\t\t(ocrW, ocrH) = (0, 0)\n",
        "\n",
        "        # если изображение визитной карточки\n",
        "        # не пустое, то возьмем его размеры\n",
        "        # if the card image is not empty, grab its dimensions\n",
        "\t\tif card is not None:\n",
        "\t\t\t(cardH, cardW) = card.shape[:2]\n",
        "\n",
        "        # аналогично, если изображение OCR не пустое, \n",
        "        # то возьмем его размеры\n",
        "        # similarly, if the OCR image is not empty, grab its\n",
        "\t\t# dimensions\n",
        "\t\tif ocr is not None:\n",
        "\t\t\t(ocrH, ocrW) = ocr.shape[:2]\n",
        "\n",
        "        # вычисляем пространственные размеры выходного кадра\n",
        "        # compute the spatial dimensions of the output frame\n",
        "\t\toutputW = max([frameW, cardW, ocrW])\n",
        "\t\toutputH = frameH + cardH + ocrH\n",
        "\n",
        "        # обновляем максимальный выходной размер,\n",
        "        # найденный на данный момент\n",
        "        # update the max output spatial dimensions found thus far\n",
        "\t\tself.maxW = max(self.maxW, outputW)\n",
        "\t\tself.maxH = max(self.maxH, outputH)\n",
        "\n",
        "        # выделяем память для выходного изображения,\n",
        "        # используя найденный выше максимальный выходной размер\n",
        "        # allocate memory of the output image using our maximum\n",
        "\t\t# spatial dimensions\n",
        "\t\toutput = np.zeros((self.maxH, self.maxW, 3), dtype=\"uint8\")\n",
        "\n",
        "        # устанавливаем рамку в выходном изображении\n",
        "        # set the frame in the output image\n",
        "\t\toutput[0:frameH, 0:frameW] = frame\n",
        "\n",
        "        # если карточка была найдена, то \n",
        "        # добавляем ее к выходному изображению\n",
        "        # if the card is not empty, add it to the output image\n",
        "\t\tif card is not None:\n",
        "\t\t\toutput[frameH:frameH + cardH, 0:cardW] = card\n",
        "\n",
        "        # если результат OCR не пуст, то \n",
        "        # добавляем его к выходному изображению\n",
        "        # if the OCR result is not empty, add it to the output image\n",
        "\t\tif ocr is not None:\n",
        "\t\t\toutput[\n",
        "\t\t\t\tframeH + cardH:frameH + cardH + ocrH,\n",
        "\t\t\t\t0:ocrW] = ocr\n",
        "\n",
        "        # возвращаем полученное изображение \n",
        "        # return the output visualization image\n",
        "\t\treturn output"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFzvIKB8ashn"
      },
      "source": [
        "Так же еще допишем функции, с которой мы уже знакомы \n",
        "\n",
        "\n",
        "Let's also add a function that we are already familiar with \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7WdRXoxRWea"
      },
      "source": [
        "def detect_blur_fft(image, size=60, thresh=10):\n",
        "    # получим размеры изображения и используем \n",
        "    # их для получения центра координат\n",
        "    # grab the dimensions of the image and use the dimensions to\n",
        "\t# derive the center (x, y)-coordinates\n",
        "\t(h, w) = image.shape\n",
        "\t(cX, cY) = (int(w / 2.0), int(h / 2.0))\n",
        "\n",
        "    # вычисляем FFT для нахождения частотного преобразования, \n",
        "    # используя встроенную функцию в numpy.\n",
        "    # Затем мы смещаем нулевую частотную к центру,\n",
        "    # где ее будет легче анализировать\n",
        "    # compute the FFT to find the frequency transform, then shift\n",
        "\t# the zero frequency component (i.e., DC component located at\n",
        "\t# the top-left corner) to the center where it will be more\n",
        "\t# easy to analyze\n",
        "\tfft = np.fft.fft2(image)\n",
        "\tfftShift = np.fft.fftshift(fft)\n",
        "\n",
        "    # обнуляем центр сдвига FFT (т.е. удаляем низкие частоты),\n",
        "    # применяем обратный сдвиг так, а затем применяем\n",
        "    # обратное FFT\n",
        "    # zero-out the center of the FFT shift (i.e., remove low\n",
        "\t# frequencies), apply the inverse shift such that the DC\n",
        "\t# component once again becomes the top-left, and then apply\n",
        "\t# the inverse FFT\n",
        "\tfftShift[cY - size:cY + size, cX - size:cX + size] = 0\n",
        "\tfftShift = np.fft.ifftshift(fftShift)\n",
        "\trecon = np.fft.ifft2(fftShift)\n",
        "\n",
        "    # вычисляем спектр магнитуд восстановленного изображения,\n",
        "    # затем вычисляем их среднее значение \n",
        "    # compute the magnitude spectrum of the reconstructed image,\n",
        "\t# then compute the mean of the magnitude values\n",
        "\tmagnitude = 20 * np.log(np.abs(recon))\n",
        "\tmean = np.mean(magnitude)\n",
        "\n",
        "    # изображение будет считаться \"размытым\", если \n",
        "    # среднее значение величин меньше порогового значения\n",
        "    # the image will be considered \"blurry\" if the mean value of the\n",
        "\t# magnitudes is less than the threshold value\n",
        "\treturn (mean, mean <= thresh)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-troyPNkSDFR"
      },
      "source": [
        "И еще одна \n",
        "\n",
        "And another one. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtOplkw_SCo3"
      },
      "source": [
        "# дополнительная функция для очистки текста\n",
        "# additional function to clear text\n",
        "def cleanup_text(text):\n",
        "    # вырезаем не-ASCII текст, чтобы мы могли нарисовать его на изображении \n",
        "    # с помощью OpenCV\n",
        "    # strip out non-ASCII text so we can draw the text on the image\n",
        "\t# using OpenCV\n",
        "\treturn \"\".join([c if (ord(c) < 128)  or ((ord(c) > 1039) and (ord(c) < 1104))  else \"\" for c in text]).strip()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zATX9SgsRcDj"
      },
      "source": [
        "Переходим к основному блоку. Реализуем скрипт, который будет распознавать видео \n",
        "\n",
        "Let's move on to the main block. Let's implement a script that will recognize video \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "au-jr-JDawfm"
      },
      "source": [
        "# создадим отдельный словарь с аргументами \n",
        "# create a separate dictionary with arguments \n",
        "\n",
        "args = {\n",
        "  \"input\" : \"video/ex1.MOV\", # путь к входному видео \n",
        "                                # input video path \n",
        "  \"output\": 'output/out.avi', # путь к выходному видео\n",
        "                        # the way to the output video\n",
        "  \"min-conf\": 50, #  минимальное значение достоверности для фильтрации слабого обнаружения текста\n",
        "                    # minimum validity value to filter out weak text detection\n",
        "}\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9X6BT9-8xQiI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0de4db1-8982-4957-936d-c13c8985a493"
      },
      "source": [
        "# инициализируем наш констуктор вывода видео OCR, используемый \n",
        "# для простой визуализации вывода на экран\n",
        "# initialize our video OCR output builder used to easily visualize\n",
        "# output to our screen\n",
        "outputBuilder = None\n",
        "\n",
        "# инициализируем переменную, которая будет записывать \n",
        "# выходное видео  и переменные с размерами выходного кадра\n",
        "# initialize our output video writer along with the dimensions of the\n",
        "# output frame\n",
        "writer = None\n",
        "outputW = None\n",
        "outputH = None\n",
        "\n",
        "# берем видеофайл\n",
        "# take a video file\n",
        "print(\"[INFO] opening video file...\")\n",
        "vs = cv2.VideoCapture(args[\"input\"])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] opening video file...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVlHPAdjas58"
      },
      "source": [
        "# перебираем кадры из видеопотока\n",
        "# loop over frames from the video stream\n",
        "while True:\n",
        "    # захватываем следующий кадр и обрабатываем\n",
        "    # grab the next frame and handle if we are reading from either\n",
        "\t# a webcam or a video file\n",
        "\torig = vs.read()\n",
        "\torig = orig[1]\n",
        "\n",
        "    # если мы еще просматриваем видео, но не \n",
        "    # захватили ни одного кадра, значит,\n",
        "    # мы достигли конца видео\n",
        "    # if we are viewing a video and we did not grab a frame then we\n",
        "\t# have reached the end of the video\n",
        "\tif orig is None:\n",
        "\t\tbreak\n",
        "\n",
        "    # изменяем размер рамки и вычисляем отношение\n",
        "    # *новой* ширины к *старой* ширине\n",
        "    # resize the frame and compute the ratio of the *new* width to\n",
        "\t# the *old* width\n",
        "\tframe = imutils.resize(orig, width=600)\n",
        "\tratio = orig.shape[1] / float(frame.shape[1])\n",
        "\n",
        "    # если наш outputBuilder для OCR видео = None, \n",
        "    # то инициализируем его\n",
        "    # if our video OCR output builder is None, initialize it\n",
        "\tif outputBuilder is None:\n",
        "\t\toutputBuilder = VideoOCROutputBuilder(frame)\n",
        "\n",
        "    # инициализируем место для распознанной визитной \n",
        "    # карточки и ее OCR\n",
        "    # initialize our card and OCR output ROIs\n",
        "\tcard = None\n",
        "\tocr = None\n",
        "\n",
        "    # преобразуем кадр в в серые оттенки\n",
        "    # и определяем, считается ли кадр размытым или нет (на основе уже\n",
        "    #  написанной нами функцией)\n",
        "    # convert the frame to grayscale and detect if the frame is\n",
        "\t# considered blurry or not\n",
        "\tgray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\t(mean, blurry) = detect_blur_fft(gray, thresh=15)\n",
        "\n",
        "    # выводим прямо поверх картинки, размыт кадр или нет\n",
        "    # draw whether or not the frame is blurry\n",
        "\tcolor = (0, 0, 255) if blurry else (0, 255, 0)\n",
        "\ttext = \"Blurry ({:.4f})\" if blurry else \"Not Blurry ({:.4f})\"\n",
        "\ttext = text.format(mean)\n",
        "\tcv2.putText(frame, text, (10, 25), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "\t\t0.7, color, 2)\n",
        "\n",
        "    # продолжаем обрабатывать кадр для OCR, \n",
        "    # только если изображение *не* размыто\n",
        "    # only continue to process the frame for OCR if the image is\n",
        "\t# *not* blurry\n",
        "\tif not blurry:\n",
        "        # слегка размываем полутоновое изображение,\n",
        "        # а затем выполняем определение краев\n",
        "        # blur the grayscale image slightly and then perform edge\n",
        "\t\t# detection\n",
        "\t\tblurred = cv2.GaussianBlur(gray, (5, 5,), 0)\n",
        "\t\tedged = cv2.Canny(blurred, 75, 200)\n",
        "\n",
        "        # находим контуры на карте граней и сортируем их по размеру \n",
        "        # в порядке убывания, сохраняя только самые крупные из них\n",
        "        # find contours in the edge map and sort them by size in\n",
        "\t\t# descending order, keeping only the largest ones\n",
        "\t\tcnts = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL,\n",
        "\t\t\tcv2.CHAIN_APPROX_SIMPLE)\n",
        "\t\tcnts = imutils.grab_contours(cnts)\n",
        "\t\tcnts = sorted(cnts, key=cv2.contourArea, reverse=True)[:5]\n",
        "\n",
        "        # инициализируем контур, который соответствует\n",
        "        # контуру визитной карточки\n",
        "        # initialize a contour that corresponds to the business card\n",
        "\t\t# outline\n",
        "\t\tcardCnt = None\n",
        "\n",
        "        # проходимся по контурам\n",
        "        # loop over the contours\n",
        "\t\tfor c in cnts:\n",
        "            # аппроксимируем контур\n",
        "            # approximate the contour\n",
        "\t\t\tperi = cv2.arcLength(c, True)\n",
        "\t\t\tapprox = cv2.approxPolyDP(c, 0.02 * peri, True)\n",
        "\n",
        "            # если наш приближенный контур имеет четыре точки,\n",
        "            #  то можно считать, что мы нашли контур визитной карточки\n",
        "            # if our approximated contour has four points, then we\n",
        "\t\t\t# can assume we have found the outline of the business\n",
        "\t\t\t# card\n",
        "\t\t\tif len(approx) == 4:\n",
        "\t\t\t\tcardCnt = approx\n",
        "\t\t\t\tbreak\n",
        "\n",
        "        # убедились, что контур визитной карточки был найден\n",
        "        # ensure that the business card contour was found\n",
        "\t\tif cardCnt is not None:\n",
        "            # рисуем контур визитной карточки на рамке, чтобы \n",
        "            # убедиться, что она была обнаружена правильно\n",
        "            # draw the outline of the business card on the frame so\n",
        "\t\t\t# we visually verify that the card was detected correctly\n",
        "\t\t\tcv2.drawContours(frame, [cardCnt], -1, (0, 255, 0), 3)\n",
        "\n",
        "            # применим преобразование перспективы в четырех точках к \n",
        "            # *оригинальному* кадру, чтобы получить вид визитной карточки\n",
        "            # сверху \n",
        "            # apply a four point perspective transform to the\n",
        "\t\t\t# *original* frame to obtain a top-down birds eye\n",
        "\t\t\t# view of the business card\n",
        "\t\t\tcard = four_point_transform(orig,\n",
        "\t\t\t\tcardCnt.reshape(4, 2) * ratio)\n",
        "\n",
        "            # выделяем память для нашей выходной визуализации OCR\n",
        "            # allocate memory for our output OCR visualization\n",
        "\t\t\tocr = np.zeros(card.shape, dtype=\"uint8\")\n",
        "\n",
        "            # меняем  канал  визитной карточки и распознаем ее\n",
        "            # swap channel ordering for the business card and OCR it\n",
        "\t\t\trgb = cv2.cvtColor(card, cv2.COLOR_BGR2RGB)\n",
        "\t\t\tresults = pytesseract.image_to_data(rgb,\n",
        "\t\t\t\toutput_type=Output.DICT, lang='eng')\n",
        "\n",
        "            # перебираем каждую отдельную текстовую локализацию\n",
        "            # loop over each of the individual text localizations\n",
        "\t\t\tfor i in range(0, len(results[\"text\"])):\n",
        "                # извлекаем координаты ограничивающей \n",
        "                # рамки текстовой области из текущего результата\n",
        "                # extract the bounding box coordinates of the text\n",
        "\t\t\t\t# region from the current result\n",
        "\t\t\t\tx = results[\"left\"][i]\n",
        "\t\t\t\ty = results[\"top\"][i]\n",
        "\t\t\t\tw = results[\"width\"][i]\n",
        "\t\t\t\th = results[\"height\"][i]\n",
        "\n",
        "                # извлекаем сам текст OCR вместе с\n",
        "                # вероятностью в локализации текста\n",
        "                # extract the OCR text itself along with the\n",
        "\t\t\t\t# confidence of the text localization\n",
        "\t\t\t\ttext = results[\"text\"][i]\n",
        "\t\t\t\tconf = int(results[\"conf\"][i])\n",
        "\n",
        "                # отфильтровываем текстовые локализации со слабым доверием\n",
        "                # filter out weak confidence text localizations\n",
        "\t\t\t\tif conf > args[\"min-conf\"]:\n",
        "                    # обрабатываем текст, удаляя из него символы,\n",
        "                    # не являющиеся символами ASCII\n",
        "                    # process the text by stripping out non-ASCII\n",
        "\t\t\t\t\t# characters\n",
        "\t\t\t\t\ttext = cleanup_text(text)\n",
        "\n",
        "                    # если очищенный текст не пустой, \n",
        "                    # то рисуем ограничивающую рамку вокруг текста\n",
        "                    # вместе с самим текстом\n",
        "                    # if the cleaned up text is not empty, draw a\n",
        "\t\t\t\t\t# bounding box around the text along with the\n",
        "\t\t\t\t\t# text itself\n",
        "\t\t\t\t\tif len(text) > 0:\n",
        "\t\t\t\t\t\tcv2.rectangle(card, (x, y), (x + w, y + h),\n",
        "\t\t\t\t\t\t\t(0, 255, 0), 2)\n",
        "\t\t\t\t\t\tcv2.putText(ocr,  text , (x, y - 10),\n",
        "\t\t\t\t\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
        "\t\t\t\t\t\t\t(0, 0, 255), 1)\n",
        "\n",
        "    # создаем окончательную визуализацию выходного видео\n",
        "    # build our final video OCR output visualization\n",
        "\toutput = outputBuilder.build(frame, card, ocr)\n",
        "\n",
        "    # проверяем, стоит ли записывать видео в новый файл \n",
        "    # check if the video writer is None *and* an output video file\n",
        "\t# path was supplied\n",
        "\tif args[\"output\"] is not None and writer is None:\n",
        "        # получаем размеры выходного кадра и \n",
        "        # инициализируем VideoWriter\n",
        "        # grab the output frame dimensions and initialize our video\n",
        "\t\t# writer\n",
        "\t\t(outputH, outputW) = output.shape[:2]\n",
        "\t\tfourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
        "\t\twriter = cv2.VideoWriter(args[\"output\"], fourcc, 27,\n",
        "\t\t\t(outputW, outputH), True)\n",
        "\n",
        "    # если writer не пуст, то нужно записать выходную \n",
        "    # визуализацию видео OCR на диск\n",
        "    # if the writer is not None, we need to write the output video\n",
        "\t# OCR visualization to disk\n",
        "\tif writer is not None:\n",
        "        # принудительно изменяем размер визуализации\n",
        "        # OCR видео в соответствии с размерами выходного видео\n",
        "        # force resize the video OCR visualization to match the\n",
        "\t\t# dimensions of the output video\n",
        "\t\toutputFrame = cv2.resize(output, (outputW, outputH))\n",
        "\t\twriter.write(outputFrame)\n",
        "\n",
        "\n",
        "vs.release()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKoyzY36pWgz"
      },
      "source": [
        "Видео воспроизводить будем по следующему принципу:\n",
        "\n",
        "We will play the video in the following way:\n",
        "\n",
        "- получаем видео в формате avi. get video in avi format.\n",
        "- конвертирую его в mp4. convert it to mp4 .\n",
        "- воспроизводим с помощью модуля kora. play it using the kora module.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWKb0TD9Smbw",
        "outputId": "a0c7b76a-2184-43f5-c6dd-fe292d70d17c"
      },
      "source": [
        "!pip install -U kora"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kora\n",
            "  Downloading kora-0.9.19-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 4.3 MB/s \n",
            "\u001b[?25hCollecting fastcore\n",
            "  Downloading fastcore-1.3.27-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 2.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from kora) (5.5.0)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (from fastcore->kora) (21.1.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from fastcore->kora) (21.3)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->kora) (0.8.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->kora) (57.4.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->kora) (4.4.2)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->kora) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->kora) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->kora) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->kora) (1.0.18)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->kora) (5.1.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->kora) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->kora) (1.15.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->fastcore->kora) (3.0.7)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->kora) (0.7.0)\n",
            "Installing collected packages: fastcore, kora\n",
            "Successfully installed fastcore-1.3.27 kora-0.9.19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvUVGL8XNqNZ",
        "outputId": "f7d1d714-1923-41d8-e338-5d4a782babcf"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "blur_detection\thelpers.py  ocr_video.py  output  video  video_ocr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsrqnOtCSmb1",
        "outputId": "5ecac048-5456-4236-aa34-40a350a1d331"
      },
      "source": [
        "!ffmpeg -i output/out.avi output/out.mp4"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers\n",
            "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
            "  libavutil      55. 78.100 / 55. 78.100\n",
            "  libavcodec     57.107.100 / 57.107.100\n",
            "  libavformat    57. 83.100 / 57. 83.100\n",
            "  libavdevice    57. 10.100 / 57. 10.100\n",
            "  libavfilter     6.107.100 /  6.107.100\n",
            "  libavresample   3.  7.  0 /  3.  7.  0\n",
            "  libswscale      4.  8.100 /  4.  8.100\n",
            "  libswresample   2.  9.100 /  2.  9.100\n",
            "  libpostproc    54.  7.100 / 54.  7.100\n",
            "Input #0, avi, from 'output/out.avi':\n",
            "  Metadata:\n",
            "    encoder         : Lavf58.35.100\n",
            "  Duration: N/A, start: 0.000000, bitrate: N/A\n",
            "    Stream #0:0: Video: mjpeg (MJPG / 0x47504A4D), yuvj420p(pc, bt470bg/unknown/unknown), 600x752, 27 fps, 27 tbr, 27 tbn, 27 tbc\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (mjpeg (native) -> h264 (libx264))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;36m[libx264 @ 0x563255b05e00] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
            "\u001b[1;36m[libx264 @ 0x563255b05e00] \u001b[0mprofile High, level 3.1\n",
            "\u001b[1;36m[libx264 @ 0x563255b05e00] \u001b[0m264 - core 152 r2854 e9a5903 - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=3 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
            "Output #0, mp4, to 'output/out.mp4':\n",
            "  Metadata:\n",
            "    encoder         : Lavf57.83.100\n",
            "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuvj420p(pc), 600x752, q=-1--1, 27 fps, 13824 tbn, 27 tbc\n",
            "    Metadata:\n",
            "      encoder         : Lavc57.107.100 libx264\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
            "\u001b[0;36m[mjpeg @ 0x563255b04f00] \u001b[0m\u001b[1;31moverread 5\n",
            "\u001b[0m\u001b[0;36m[mjpeg @ 0x563255b04f00] \u001b[0m\u001b[0;33mEOI missing, emulating\n",
            "\u001b[0mframe=  315 fps= 19 q=-1.0 Lsize=    1366kB time=00:00:11.55 bitrate= 968.4kbits/s speed=0.702x    \n",
            "video:1362kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.329424%\n",
            "\u001b[1;36m[libx264 @ 0x563255b05e00] \u001b[0mframe I:3     Avg QP:18.19  size: 24524\n",
            "\u001b[1;36m[libx264 @ 0x563255b05e00] \u001b[0mframe P:84    Avg QP:22.40  size:  8944\n",
            "\u001b[1;36m[libx264 @ 0x563255b05e00] \u001b[0mframe B:228   Avg QP:27.14  size:  2494\n",
            "\u001b[1;36m[libx264 @ 0x563255b05e00] \u001b[0mconsecutive B-frames:  1.6%  3.8%  5.7% 88.9%\n",
            "\u001b[1;36m[libx264 @ 0x563255b05e00] \u001b[0mmb I  I16..4: 15.1% 75.6%  9.4%\n",
            "\u001b[1;36m[libx264 @ 0x563255b05e00] \u001b[0mmb P  I16..4:  6.0% 18.4%  1.2%  P16..4: 13.8%  7.4%  4.2%  0.0%  0.0%    skip:49.1%\n",
            "\u001b[1;36m[libx264 @ 0x563255b05e00] \u001b[0mmb B  I16..4:  1.7%  4.3%  0.2%  B16..8: 20.3%  5.1%  0.9%  direct: 1.1%  skip:66.5%  L0:46.4% L1:45.0% BI: 8.6%\n",
            "\u001b[1;36m[libx264 @ 0x563255b05e00] \u001b[0m8x8 transform intra:71.5% inter:73.5%\n",
            "\u001b[1;36m[libx264 @ 0x563255b05e00] \u001b[0mcoded y,uvDC,uvAC intra: 32.9% 17.6% 4.4% inter: 7.0% 6.7% 4.5%\n",
            "\u001b[1;36m[libx264 @ 0x563255b05e00] \u001b[0mi16 v,h,dc,p: 53% 37%  7%  3%\n",
            "\u001b[1;36m[libx264 @ 0x563255b05e00] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 27% 27% 41%  2%  1%  0%  1%  1%  1%\n",
            "\u001b[1;36m[libx264 @ 0x563255b05e00] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 29% 40% 18%  2%  2%  2%  3%  2%  2%\n",
            "\u001b[1;36m[libx264 @ 0x563255b05e00] \u001b[0mi8c dc,h,v,p: 68% 20% 11%  1%\n",
            "\u001b[1;36m[libx264 @ 0x563255b05e00] \u001b[0mWeighted P-Frames: Y:1.2% UV:0.0%\n",
            "\u001b[1;36m[libx264 @ 0x563255b05e00] \u001b[0mref P L0: 52.5% 18.0% 20.1%  9.4%\n",
            "\u001b[1;36m[libx264 @ 0x563255b05e00] \u001b[0mref B L0: 80.5% 15.6%  3.8%\n",
            "\u001b[1;36m[libx264 @ 0x563255b05e00] \u001b[0mref B L1: 92.0%  8.0%\n",
            "\u001b[1;36m[libx264 @ 0x563255b05e00] \u001b[0mkb/s:955.59\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66EWitLKTclb"
      },
      "source": [
        "Для корректной работы нам необходимо подключить модуль kora, который просит подключиться к гугл диску. Для этого мы при запуске блока кода ниже должны перейти по появившейся ссылки и после соглашения скопировать ссылку и вставить сюда. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 823
        },
        "id": "qRxi3ejOOIsn",
        "outputId": "74fef9b8-eeb4-45d8-d663-a05c91ef2dbd"
      },
      "source": [
        "from kora.drive import upload_public\n",
        "url = upload_public('output/out.mp4')\n",
        "\n",
        "from IPython.display import HTML\n",
        "HTML(f\"\"\"<video src={url} width=640 controls/>\"\"\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<video src=https://drive.google.com/uc?id=1erhRj24iu944G29JbqYIOHHj256yNY3M width=640 controls/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}